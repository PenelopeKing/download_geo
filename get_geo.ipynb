{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96317d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.9/site-packages/pysradb/utils.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "from geofetch import Geofetcher\n",
    "import pysradb.sraweb\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import sys\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, Dropdown, Button, Text\n",
    "import traitlets\n",
    "from traitlets import dlink\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e68a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes folder for files to download\n",
    "curr_path = os.getcwd()\n",
    "download_path = os.path.join(curr_path, 'downloaded_files')\n",
    "try: \n",
    "    os.mkdir(download_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593199d0",
   "metadata": {},
   "source": [
    "#### GSE gathering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f2fe2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_gse(super_gse):\n",
    "    \"\"\"\n",
    "    Using a super series accession code, gets the sub series accession codes via webscrape\n",
    "    \n",
    "    input: super_gse (str)\n",
    "    output: sub_gse (str list)\n",
    "    \"\"\"\n",
    "    \n",
    "    import requests\n",
    "    import time\n",
    "    from bs4 import BeautifulSoup\n",
    "    import regex as re\n",
    "    \n",
    "    # webscrapping\n",
    "    base_link = 'https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc='\n",
    "    response = requests.get(base_link+super_gse)\n",
    "    start = time.time()\n",
    "    soup = BeautifulSoup(response.text, features='lxml')\n",
    "    \n",
    "    pattern = re.compile(r'^GSE') # pattern looking for in text\n",
    "    href = '/geo/query/acc.cgi?acc' # pattern looking for in links\n",
    "    a_href = soup.find_all('a', href=True, text=pattern)\n",
    "    \n",
    "    # finds sub gse accession codes\n",
    "    sub_gse = []\n",
    "    for sub in a_href:\n",
    "        temp = sub.text.strip().upper()\n",
    "        if temp != super_gse:\n",
    "            sub_gse.append(temp)\n",
    "    \n",
    "    # to accomodate robots.txt, must wait 5 seconds before web scrape again\n",
    "    end = time.time()\n",
    "    to_wait = 5 - (end-start)\n",
    "    if to_wait < 0:\n",
    "        to_wait = 0\n",
    "    time.sleep(to_wait)\n",
    "    \n",
    "    return sub_gse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab534d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_files(gse):\n",
    "    \"\"\"\n",
    "    uses geofetch to get supplemental data files using a GSE accession #\n",
    "    input: gse (str)\n",
    "    output: df of supp data file info\n",
    "    \"\"\"\n",
    "    # get supplemental data\n",
    "    geof = Geofetcher(processed=True, data_source='all', just_metadata=True, discard_soft = True)\n",
    "    proj = geof.get_projects(gse)\n",
    "    \n",
    "    # check if supplemental data exists\n",
    "    gse_series = gse + '_series'\n",
    "    if gse_series in proj.keys():\n",
    "        df = proj[gse_series].sample_table\n",
    "        # clean columns, names for consistency when concatenating df's\n",
    "        keep = ['file','sample_name', 'series_sample_organism', 'series_sample_id', 'file_url', 'series_overall_design','series_type']\n",
    "        df= df[keep]\n",
    "        df['study_alias'] = [gse] * df.shape[0]\n",
    "        return df.rename(columns={'series_sample_organism': 'organism_name',\n",
    "                                 'sample_name':'title',\n",
    "                                 'series_sample_id':'GSM',\n",
    "                                 'series_overall_design':'description'}).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7738b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptions(gse, where_save):\n",
    "    \"\"\"\n",
    "    Gets a compliation of metadata for selection\n",
    "    input: gse (str), where_save (str)\n",
    "    output: df & downloads .csv\n",
    "    \"\"\"\n",
    "    ### input validation ###\n",
    "    gse = gse.strip()\n",
    "    gse = gse.upper()\n",
    "    print('Downloading {}...'.format(gse))\n",
    "    if (not type(gse) is str) or (gse[:3] != 'GSE'):\n",
    "        print(\"Incorrect formatting! Input must be a string starting with GSE followed by numbers\")\n",
    "        return\n",
    "    if (gse == ''):\n",
    "        print(\"please enter input\")\n",
    "        return\n",
    "    ### end of validation ###\n",
    "    \n",
    "    def clean_df(df):\n",
    "        cols_keep = ['run_accession', 'experiment_title', 'study_alias', 'organism_name', 'experiment_alias', 'experiment_desc','total_spots', 'total_size', 'run_total_spots','run_total_bases']\n",
    "        output =  df[cols_keep].rename(columns={'run_accession':'file',\n",
    "                                    'experiment_title':'title', \n",
    "                                     'experiment_desc': 'description',\n",
    "                                    'experiment_alias' : 'GSM'})\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    # gets relevant information\n",
    "    geo_helper = pysradb.sraweb.SRAweb()\n",
    "    gsm_srx = geo_helper.gse_to_gsm(gse)\n",
    "    \n",
    "    try:\n",
    "        srp = geo_helper.gse_to_srp(gse)['study_accession'][0] # gets the srp code\n",
    "    \n",
    "    # catches error that happens if gse is a super_gse, performs webscraping\n",
    "    except ValueError:\n",
    "        sub_gse_lst = find_sub_gse(gse)\n",
    "    else:\n",
    "        sub_gse_lst = [gse]\n",
    "    \n",
    "    finally:\n",
    "        output = pd.DataFrame()\n",
    "        series_df = pd.DataFrame()\n",
    "        \n",
    "        for sub_gse in sub_gse_lst:\n",
    "            # for sra data\n",
    "            srp = geo_helper.gse_to_srp(sub_gse)['study_accession'][0] # gets the srp code\n",
    "            df = geo_helper.sra_metadata(srp) # contains meta info of data for gse\n",
    "            sub_output = pd.merge(gsm_srx, df)\n",
    "            output = pd.concat([output, sub_output], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "            # for supplemental data\n",
    "            sub_series = get_series_files(sub_gse)\n",
    "            series_df = pd.concat([series_df, sub_series], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "        \n",
    "        # clean output data frame\n",
    "        output =  clean_df(output)\n",
    "        if series_df.shape != (0,0):\n",
    "            # concat srr (fastq) and supp series data frames\n",
    "            output = pd.concat([output, series_df], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "        # turn output into csv, download it\n",
    "        filename = gse + '_metadata.csv'\n",
    "        filename = os.path.join(where_save, filename)\n",
    "        output.to_csv(filename)\n",
    "        print('\\nA .csv version of this dataframe has been saved at {}'.format(filename))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b68cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(csv, path):\n",
    "    \"\"\"\n",
    "    Downloads files that partial csv contains (all rows)\n",
    "    input: csv path and download path\n",
    "    output: none\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # split btw fastq and supplemental files\n",
    "    lst = df['file']\n",
    "    srr_to_download = lst[lst.apply(lambda x: 'SRR' in x)].to_list()\n",
    "    supp_to_download = lst[~lst.apply(lambda x: 'SRR' in x)].index.to_list()\n",
    "    \n",
    "    # makes folder for files to download\n",
    "    download_path = os.path.join(path, 'downloaded_files')\n",
    "    try: \n",
    "        os.mkdir(download_path)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ### downloading supplemental files ###\n",
    "    print('Downloading supplemental files...\\n')\n",
    "    for supp_idx in supp_to_download:\n",
    "        download_link = 'https://' + df.iloc[supp_idx]['file_url'].split(\"//\")[-1]\n",
    "        file_name = os.path.join(download_path, download_link.split(\"/\")[-1])\n",
    "        urllib.request.urlretrieve(download_link, file_name)\n",
    "\n",
    "    ### downloading fastq files ###\n",
    "    print('Downloading FASTQ files... this make take a bit...\\n')\n",
    "    for srr in srr_to_download:\n",
    "        # prefetch srr files for quicker download\n",
    "        download_path = os.path.join(path, 'downloaded_files')\n",
    "        pf_cmd = 'prefetch {} -O {}'.format(srr, download_path)\n",
    "        print(pf_cmd)\n",
    "        os.system(pf_cmd)\n",
    "        # download fastq\n",
    "        path = os.path.join(download_path,os.path.join(srr, srr+'.sra'))\n",
    "        fastq_cmd = 'fasterq-dump --outdir {} {}'.format(os.path.join(download_path, srr), path)\n",
    "        print(fastq_cmd)\n",
    "        os.system(fastq_cmd)\n",
    "\n",
    "    print('\\nComplete\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45ab96",
   "metadata": {},
   "source": [
    "### GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "967b94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pretty_table(csv_path):\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Display the DataFrame using IPython display\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885e3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum_k(directory):\n",
    "    # Get all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    # Filter for files starting with Partial_\n",
    "    partial_files = [f for f in files if f.startswith(\"Partial_\")]\n",
    "    if len(partial_files) == 0:\n",
    "        return 0\n",
    "   \n",
    "    # Extract the k values from each filename\n",
    "    k_values = [int(f.split(\"_\")[1]) for f in partial_files]\n",
    "   \n",
    "    # Return the maximum k value\n",
    "    return max(k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4f17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_and_select_rows(csv_path, column_name):\n",
    "    \"\"\"\n",
    "    modified original method for binder use\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.rename(columns={'Unnamed: 0':'index'})\n",
    "\n",
    "    # Create a dropdown widget to select items from the column\n",
    "    options_list = [''] +df[column_name].unique().tolist()\n",
    "    dropdown = Dropdown(options= options_list)\n",
    "    \n",
    "    save_path_input = download_path\n",
    "    \n",
    "    # Create a button widget for saving and downloading\n",
    "    button = Button(description='Save and Download')\n",
    "    \n",
    "    # Create an empty DataFrame to store selected rows\n",
    "    selected_rows_df = pd.DataFrame()\n",
    "    \n",
    "    # Function to display filtered rows based on the selected item\n",
    "    def display_filtered_rows(selected_item):\n",
    "        nonlocal selected_rows_df\n",
    "        \n",
    "        # Filter the DataFrame based on the selected item\n",
    "        filtered_df = df[df[column_name] == selected_item]\n",
    "        \n",
    "        # Append the filtered rows to the selected rows DataFrame\n",
    "        selected_rows_df = pd.concat([selected_rows_df, filtered_df], ignore_index=True)\n",
    "        \n",
    "        # Display the selected rows\n",
    "        display(filtered_df)\n",
    "        \n",
    "        # Print the growing list of selected rows\n",
    "        print(\"Selected Rows:\")\n",
    "        for i in range(len(selected_rows_df)):\n",
    "            row_entry = str(selected_rows_df.loc[i, column_name])[:60] + '...' if len(str(selected_rows_df.loc[i, column_name])) > 60 else str(selected_rows_df.loc[i, column_name])\n",
    "            print(f\"Row {i+1}: {row_entry}\")\n",
    "        print()  # Print an empty line for separation\n",
    "        \n",
    "    # Function to handle the save and download button click event\n",
    "    def save_and_download(_):\n",
    "        nonlocal selected_rows_df\n",
    "        \n",
    "        # Check if any rows are selected\n",
    "        if selected_rows_df.empty:\n",
    "            print(\"No rows are selected.\")\n",
    "            return\n",
    "        \n",
    "        # Get the save path directory from the input text widget\n",
    "        save_directory = save_path_input\n",
    "        \n",
    "        # Get the directory and file part of the original CSV file path\n",
    "        directory = os.path.dirname(csv_path)\n",
    "        file_name = os.path.basename(csv_path)\n",
    "        file_part = os.path.splitext(file_name)[0]\n",
    "        file_extension = os.path.splitext(file_name)[1]\n",
    "        \n",
    "        # Find the next available k for the Partial_k file name in the directory\n",
    "        k = get_maximum_k(save_directory)\n",
    "        \n",
    "        # Create the Partial_k file name\n",
    "        partial_k_filename = f\"Partial_{k+1}_{file_part}.csv\"\n",
    "        \n",
    "        # Combine the save directory, partial_k file name, and save path to form the final save path\n",
    "        save_path = os.path.join(save_directory, partial_k_filename)\n",
    "        \n",
    "        # Write the selected rows to a new CSV file\n",
    "        selected_rows_df.to_csv(save_path, index=False)\n",
    "        \n",
    "        # Print the save path and exit the program\n",
    "        print(f\"Saved CSV file: {save_path}\")\n",
    "        \n",
    "        # download the files in partial csv\n",
    "        download_files(save_path, save_directory)\n",
    "        \n",
    "        # Clear the selected rows DataFrame for the next iteration\n",
    "        selected_rows_df = pd.DataFrame()\n",
    "    \n",
    "    # Connect the dropdown widget to the display function\n",
    "    interact(display_filtered_rows, selected_item=dropdown)\n",
    "    \n",
    "    # Connect the button widget to the save and download function\n",
    "    button.on_click(save_and_download)\n",
    "    \n",
    "    # Display the input text widget and button widget\n",
    "    display(button) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4461774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes selection easier for user end on binder\n",
    "def preview_data_visual(gse):\n",
    "    '''\n",
    "    input: gse (str) - GSE accession number\n",
    "    '''\n",
    "    get_descriptions(gse, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "545dd78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_csv(csv_path):\n",
    "    \"\"\"\n",
    "    displays csv for binder\n",
    "    input: csv_path (str) - path of csv to turn into a dataframe\n",
    "    \"\"\"\n",
    "    return pd.read_csv(csv_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2c711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b6a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46bdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
